---
title: "Turboveg for R"
format: html
---

Aim of this tutorial is to show you step by step how to import the data from Turboveg to R and prepare it for further analyses.

## 1.1 Turboveg data format

Turboveg for Windows is a program designed for the storage, selection, and export of vegetation plot data (relev√©s).The information is divided among several files that are matched by either species ID or releve ID. Within the Turboveg interface, you do not see this structure directly, but you can find it if you look in the *Turbowin* folder, subfolder *data* and particular database (see example below).

![](images/clipboard-683841843.png){width="637"}

At some point you need to export the data and process them further. This can be done for example in a specialised software called JUICE, but also directly in R.

![](images/clipboard-2303630584.png)

To get Turboveg data to R, you first need to **export the Turboveg database** to a folder where you want to process the data. This step requires **selection of all the plots** you want to export. Alternatively you can access the files directly in the main file in Turbowin, but just to warn you, if you do something wrong here, you might completely loose your data.

## 1.2 Load libraries

```{r}
#| warning: false
library(foreign)   #for reading dbf files 
library(tidyverse) #for data handling, pipes and visualisation 
library(readxl)    #for data import directly from Excel 
library(janitor)   #for unified, easy-to-handle format of variable names
```

## 1.3 Import env file = headers

**One option** is to check the exported database manually, open the file called *tvhabita.dbf* in Excel and save it as *tvhabita.xlsx* or *tvhabita.csv* (UTF 8 encoded) file into your *data* folder. Although it includes one more step outside R, it is still rather straightforward and it saves you troubles with different formats in Turboveg and in R (encoding issues).

You can then import the file as Excel

```{r}
#| eval: false
env <- read_excel("data/tvhabita.xlsx")
```

or from csv file, which is a slightly more universal option and we will use it for import of most of the files.

```{r}
env <- read_csv("data/tvhabita.csv")
```

If you check the imported names, they are rather difficult to handle.

```{r}
names(env)
```

Therefore we will directly change them to tidy names with the `clean_names` function from package `janitor`. Alternative is to rename one by one using e.g. `rename`, but here we want to save time and effort.

```{r}
env <- read_csv("data/tvhabita.csv")%>% 
  clean_names() 

tibble(env)
```

Note that the **pipe** `%>%` allows the output of a previous command to be used as input to another command instead of using nested functions. It means, pipe binds individual steps into a sequence and it reads from left to right. You can insert it into your code as `ctrl+shift+M`.

```{r}
names(env)
```

Pipe also enables me to try first what would be the output before saving the result. For example I want to select just few variables for checking, but not to overwrite the data, before I am happy with the selection. For example here I see, the habitat information is not filled (returns NAs), so I will not use it.

```{r}
env %>% 
  select(releve_nr, habitat, latitude, longitude)
```

When I am fine with the selection, I can rewrite the file

```{r}
env <- env %>% 
  select(releve_nr, coverscale,field_nr, country, author, date, syntaxon, 
         altitude, exposition, inclinatio, 
         cov_trees, cov_shrubs, cov_herbs, cov_mosses, 
         latitude, longitude, precision, bias_min, bias_gps, locality )
```

Or I can add all the steps I did so far into one pipeline and check the resulting dataset

```{r}
env <- read_csv("data/tvhabita.csv")%>% 
  clean_names() %>% 
  select(releve_nr, coverscale, field_nr, country, author, date, syntaxon, 
         altitude, exposition, inclinatio, 
         cov_trees, cov_shrubs, cov_herbs, cov_mosses, 
         latitude, longitude, precision, bias_min, bias_gps, locality ) %>% 
  glimpse()
```

And save it for easier access. Always keep releve_nr and coverscale, as you will need them later on.

```{r}
write.csv(env, "data/env.csv")
```

**\*Alternative option** is to directly import the file exported from Turboveg database named **tvhabita.dbf** using specific approach. Since dbf is a bit specific type of files and we need to use a specialised packages. Here I used **read.dbf** function from the `foreign` library.

```{r}
env_dbf <- read.dbf("data/tvhabita.dbf", as.is = F) %>%    
  clean_names()
```

Check the structure, directly in R

```{r}
view(env_dbf)
```

Get the list of the variable names

```{r}
names(env_dbf)
```

There are several issues with this type of import, as there might be different encodings used in the original files, not compatible with R. For Czech dataset I needed to further change the encoding style, so that the diacritics in text columns translates correctly.

Check where are problems with encoding, add column names to the brackets, first we need to select the columns that include text there are issues with diacritics and special symbols, e.g. remarks, locality... I specify them in the brackets and use function **iconv** to change the encoding to UTF-8. You may need to play a bit to see if it works correctly and change the original type in the from argument. I added one more line to transform the dataframe to tibble, which is the data format used in tidyverse packages.

```{r}
env_dbf %>%   
  mutate(across(c(remarks, locality, habitat, soil),
                ~ iconv(.x, from = "cp852", to = "UTF-8"))) %>%
  select(locality, soil) %>%
  as_tibble()
```

In the next step we select variables we want to keep further, which is useful, as the database structure includes also predefined variables, even if they are empty. Another advantage of select function

```{r}
env <- env_dbf %>%
  mutate(across(c(remarks, locality, habitat, soil),
                ~ iconv(.x, from = "cp852", to = "UTF-8"))) %>%
  select(releve_nr, coverscale,field_nr, country, author, date, syntaxon, 
         altitude, exposition, inclinatio, 
         cov_trees, cov_shrubs, cov_herbs, cov_mosses, 
         latitude, longitude, precision, bias_min, bias_gps, locality )%>%
  as_tibble()
```

## 1.4 Import spe file = species file in a long format

First option is again to open the *tvabund.dbf* in Excel and save it as *tvabund.csv*. And import it to our environment in R. Again, I will use the clean_names function during import, so that we have the same style of the variable names.

```{r}
tvabund <- read_csv("data/tvabund.csv") %>% 
  clean_names()
```

\*Alternative option is to read the data directly from the dbf file. In this case, it is less complicated than import of env file, as there are no text variables.

```{r}
#| eval: false
tvabund <- read.dbf("data/tvabund.dbf", as.is = F) %>% 
  clean_names()
```

```{r}
glimpse(tvabund)
```

Now we will check the data again and we see, that there are no species names, just numbers. Also the cover is given in the original codes and not in percentages. See the scheme below to understand where each piece of the information is stored.

![](images/clipboard-161695252.png)

We have to prepare these different files we need, import them and merge them.

### 1.4.1 Nomenclature

In the abund file, species numbers refer to the codes in the checklist used in the Turboveg database. To translate them into species names you will need a translation table with the original number in the database, original name in the database and the name you want to use in the analyses.

![](images/clipboard-3188719096.png)

**Preparation of translation table:** To show you how to prepare such a table I opened the checklist file, so called *species.dbf* from the folder: Turbowin/species/CzechiaSlovakia2015 and saved it here in the data folder as *species.csv* Using the following pipeline you can prepare your own translation table and add other names or information.

```{r}
nomenclature_raw <- read_csv("data/species.csv") %>%   
  clean_names() %>%   
  left_join(select(., species_nr, accepted_abbreviat = abbreviat),     
            by = c("valid_nr" = "species_nr")) %>%   
  mutate(accepted_name = if_else(synonym, accepted_abbreviat, abbreviat)) %>%
  select(-accepted_abbreviat) 
```

We will now import the nomenclature file that is already adapted for Czech flora.

```{r}
nomenclature <- read_csv("data/nomenclature_20251108.csv") %>%
  clean_names()  

tibble(nomenclature)
```

There are several advantages about this approach. First, you can adjust the nomenclature to the newest source/regional checklist etc. In our example the name in Turboveg is translated to the nomenclature presented in the recent Key of the Czech Flora, and it is named after the main editor Kaplan.

Second, I can add a concept that groups several taxa into higher units, e.g. taxa that are not easy to recognise in the field are assigned into aggregates. This is exactly the same approach as you do when you create an expert system file. Here it is even more easy to understand and much easier to change the translation when you need to fix something. The name in this file is called ESy.

Last but not least. I can directly add much more information into such table. For example status, growth form or anything else. Here we have indication if the species is nonvascular.

I might want to check how the species are translated with the use of my translation file and select just these matching rows. Either create a variable called selection indicating if the species is in the subset or not

```{r}
nomenclature_check<- nomenclature %>% 
  left_join(tvabund %>% 
              distinct(species_nr)%>%
              mutate(selection=1)) 
```

or I can even add the frequency, how many times it appears in the records of the dataset

```{r}
nomenclature_check<- nomenclature %>% 
  left_join(tvabund %>% 
              count(species_nr)) 
```

I can then write the file, make adjustments e.g. in Excel and upload it newly. Great thing is that I have indication of which species are in the dataset and I do not have to pay attention to the other rows.

```{r}
write.csv(nomenclature_check, "data/nomenclature_check.csv")
```

upload the new, adjusted file

```{r}
#| eval: false
nomenclature <- read_csv("data/nomenclature_check.csv") %>%
  clean_names()  
```

### 1.4.2 Cover

We have translation table for nomenclature, but we still need to translate cover codes to percentages. For cover translation we need to use information about cover scale (stored in the header data / tvhabita / env file) and information how to translate the values in that particular scale to percentages. The file here was prepared based on the translation of cover values in different scales to percentages following the EVA database approach. One more column was added to enable different adjustments, for example change the values for rare species etc. For any project I suggest to open the file and check if the scales you are using are there and if you agree with the translation.

```{r}
cover <- read_csv("data/cover_20230402.csv") %>% clean_names()  

tibble(cover)
```

The file here was prepared based on the translation of cover values in different scales to percentages following the EVA database approach. One more column was added to enable different adjustments, for example change the values for rare species etc. For any project I suggest to open the file and check if the scales you are using are there and if you agree with the translation.

Here I can check the different scale names included in the file

```{r}
cover %>% distinct(cover_scale_name) 
```

And I can also filter the rows of the specified scales. E.g. here I am looking for all those that start with a specific pattern "Braun"

```{r}
cover %>% 
  filter(str_starts(cover_scale_name, "Braun")) %>% 
  print(n=20)
```

### 1.4.2 Merging all files together into complete spe file

Finally, I have translation to nomenclature, to cover, so I need to put everything together.

```{r}
tvabund %>% 
  left_join(nomenclature %>% 
              select(species_nr, kaplan, expert_system, nonvascular)) %>% 
  left_join(env %>% select(releve_nr, coverscale)) %>% 
  left_join(cover %>% select(coverscale,cover_code,cover_perc))
```

The output contains these variables

```         
 "releve_nr"     "species_nr"    "cover_code"    "layer"         "orig_name"     "kaplan"        "expert_system" "nonvascular"   "coverscale"    "cover_perc"   
```

If I am satisfied with the result I assign the pipeline into the *spe* file and add one more line to select just needed variables, here I decided to use just the *expert_system name* and I renamed it directly in the `select` function

```{r}
spe<- tvabund %>% 
  left_join(nomenclature %>% 
              select(species_nr, kaplan, expert_system, nonvascular)) %>% 
  left_join(env %>% select(releve_nr, coverscale)) %>% 
  left_join(cover %>% select(coverscale,cover_code,cover_perc)) %>%
  # filter (!nonvascular=1) %>% # optional to remove nonvasculars
  select(releve_nr, species= expert_system, nonvascular, layer, cover_perc)
```

To see the result we will use view

```{r}
view(spe)
```

We can again save the final file, to be easily accessible for later

```{r}
write.csv(spe, "data/spe.csv")
```
